{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch accelerate -q\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a smaller instruction-tuned model suitable for Google Colab\n",
    "# We'll use TinyLlama-1.1B-Chat as it's relatively lightweight but still capable\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with lower precision for efficiency\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "    device_map=\"auto\",  # Automatically handle device placement\n",
    "    load_in_8bit=True if device == \"cuda\" else False  # Use 8-bit quantization if GPU available\n",
    ")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Format the prompt according to the model's expected format\n",
    "# TinyLlama uses specific tokens for chat\n",
    "prompt = \"<|system|>You are a helpful AI assistant.<|user|>How are you?<|assistant|>\"\n",
    "\n",
    "# Generate response\n",
    "print(\"\\nGenerating response...\")\n",
    "response = generator(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Extract and print the generated response\n",
    "generated_text = response[0]['generated_text']\n",
    "# Extract only the assistant's response (after the last <|assistant|> token)\n",
    "assistant_response = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "print(\"\\nResponse:\", assistant_response)\n",
    "\n",
    "# Free up memory\n",
    "del model\n",
    "del generator\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
